/*
 * @File    :  /project-3-gigi/frontend/src/mock/mock.js
 * @Author  :  xuzf
 * @Contact :  xuzhengfei-email@qq.com
 * @Create  :  2022-11-12 10:54:23
 * @Update  :  2022-12-11 12:25:17
 * @Desc    :  None
 */
//引入mockjs
import { mock } from 'mockjs';   //安装的mockjs，并不是创建的mock.js

//使用mockjs模拟数据
mock(RegExp('http://mock/api/search' + '.*'), 'get', function () { //当post或get请求到/api/data路由时Mock会拦截请求并返回上面的数据
    let hitList_new = [
        {
            id: "5",
            title: "Evaluating the factual consistency of abstractive text summarization",
            authors: ["Baoyu Jing", "Zeyu You", "Tao Yang", "Wei Fan", "Hanghang Tong"],
            abstract: "Currently used metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and a generated summary. Training data is generated by applying a series of rule-based transformations to the sentences of source documents. The factual consistency model is then trained jointly for three tasks: 1) identify whether sentences remain factually consistent after transformation, 2) extract a span in the source documents to support the consistency prediction, 3) extract a span in the summary sentence that is inconsistent if one exists. Transferring this model to summaries generated by several state-of-the art models reveals that this highly scalable approach substantially outperforms previous models, including those trained with strong supervision using standard datasets for natural language inference and fact checking. Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency.",
            doi: "10.18653/v1/2021.emnlp-main.11",
            year: 2021,
            month: 11,
            type: "Conference",
            venue: "EMNLP",
            volume: "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
            source_url: "https://aclanthology.org/2021.emnlp-main.11/",
            pdf_url: "https://aclanthology.org/2021.emnlp-main.11.pdf",
            ebook_url: "https://aclanthology.org/2021.emnlp-main.11.pdf",
            ppt_url: "https://www.slideserve.com/vera/text-summarization",
            video_url: "https://aclanthology.org/2021.emnlp-main.11.mp4",
            citations_num: 294,         // 被引次数
            // reference_list: [ref1, ref2, ...]     // 参考文献列表
        },
        {
            id: "2",
            title: "Automatic text summarization: A comprehensive survey",
            authors: ["Baoyu Jing", "Zeyu You", "Tao Yang", "Wei Fan", "Hanghang Tong"],
            abstract: "Automatic Text Summarization (ATS) is becoming much more important because of the huge amount of textual content that grows exponentially on the Internet and the various archives of news articles, scientific papers, legal documents, etc. Manual text summarization consumes a lot of time, effort, cost, and even becomes impractical with the gigantic amount of textual content. Researchers have been trying to improve ATS techniques since the 1950s. ATS approaches are either extractive, abstractive, or hybrid. The extractive approach selects the most important sentences in the input document(s) then concatenates them to form the summary. The abstractive approach represents the input document(s) in an intermediate representation then generates the summary with sentences that are different than the original sentences. The hybrid approach combines both the extractive and abstractive approaches. Despite all the proposed methods, the generated summaries are still far away from the human-generated summaries. Most researches focus on the extractive approach. It is required to focus more on the abstractive and hybrid approaches. This research provides a comprehensive survey for the researchers by presenting the different aspects of ATS: approaches, methods, building blocks, techniques, datasets, evaluation methods, and future research directions.",
            doi: "10.18653/v1/2021.emnlp-main.11",
            year: 2021,
            month: 11,
            type: "Conference",
            venue: "EMNLP",
            volume: "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
            source_url: "https://aclanthology.org/2021.emnlp-main.11/",
            pdf_url: "https://aclanthology.org/2021.emnlp-main.11.pdf",
            ebook_url: "https://aclanthology.org/2021.emnlp-main.11.pdf",
            ppt_url: "https://www.slideserve.com/vera/text-summarization",
            video_url: "https://aclanthology.org/2021.emnlp-main.11.mp4",
            citations_num: 231,         // 被引次数
            // reference_list: [ref1, ref2, ...]     // 参考文献列表
        },
        {
            id: "3",
            title: "Discourse-aware neural extractive text summarization",
            authors: ["Baoyu Jing", "Zeyu You", "Tao Yang", "Wei Fan", "Hanghang Tong"],
            abstract: "Recently BERT has been adopted for document encoding in state-of-the-art text summarization models. However, sentence-based extractive models often result in redundant or uninformative phrases in the extracted summaries. Also, long-range dependencies throughout a document are not well captured by BERT, which is pre-trained on sentence pairs instead of documents. To address these issues, we present a discourse-aware neural summarization model - DiscoBert. DiscoBert extracts sub-sentential discourse units (instead of sentences) as candidates for extractive selection on a finer granularity. To capture the long-range dependencies among discourse units, structural discourse graphs are constructed based on RST trees and coreference mentions, encoded with Graph Convolutional Networks. Experiments show that the proposed model outperforms state-of-the-art methods by a significant margin on popular summarization benchmarks compared to other BERT-base models.",
            doi: "10.18653/v1/2021.emnlp-main.11",
            year: 2021,
            month: 11,
            type: "Conference",
            venue: "EMNLP",
            volume: "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
            source_url: null,
            pdf_url: "https://aclanthology.org/2021.emnlp-main.11.pdf",
            ebook_url: "https://aclanthology.org/2021.emnlp-main.11.pdf",
            ppt_url: "https://www.slideserve.com/vera/text-summarization",
            video_url: "https://aclanthology.org/2021.emnlp-main.11.mp4",
            citations_num: 183,         // 被引次数
            // reference_list: [ref1, ref2, ...]     // 参考文献列表
        },
        {
            id: "4",
            title: "Leveraging BERT for Extractive Text Summarization on Lectures",
            authors: ["Baoyu Jing", "Zeyu You", "Tao Yang", "Wei Fan", "Hanghang Tong"],
            abstract: "In the last two decades, automatic extractive text summarization on lectures has demonstrated to be a useful tool for collecting key phrases and sentences that best represent the content. However, many current approaches utilize dated approaches, producing sub-par outputs or requiring several hours of manual tuning to produce meaningful results. Recently, new machine learning architectures have provided mechanisms for extractive summarization through the clustering of output embeddings from deep learning models. This paper reports on the project called Lecture Summarization Service, a python based RESTful service that utilizes the BERT model for text embeddings and KMeans clustering to identify sentences closes to the centroid for summary selection. The purpose of the service was to provide students a utility that could summarize lecture content, based on their desired number of sentences. On top of the summary work, the service also includes lecture and summary management, storing content on the cloud which can be used for collaboration. While the results of utilizing BERT for extractive summarization were promising, there were still areas where the model struggled, providing feature research opportunities for further improvement.",
            doi: "10.18653/v1/2021.emnlp-main.11",
            year: 2021,
            month: 11,
            type: "Conference",
            venue: "EMNLP",
            volume: "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
            source_url: "https://aclanthology.org/2021.emnlp-main.11/",
            pdf_url: "https://aclanthology.org/2021.emnlp-main.11.pdf",
            ebook_url: "https://aclanthology.org/2021.emnlp-main.11.pdf",
            ppt_url: "https://www.slideserve.com/vera/text-summarization",
            video_url: "https://aclanthology.org/2021.emnlp-main.11.mp4",
            citations_num: 165,         // 被引次数
            // reference_list: [ref1, ref2, ...]     // 参考文献列表
        },
        {
            id: "1",
            title: "Multiplex Graph Neural Network for Extractive Text Summarization",
            authors: ["Baoyu Jing", "Zeyu You", "Tao Yang", "Wei Fan", "Hanghang Tong"],
            abstract: "Extractive text summarization aims at extracting the most representative sentences from a given document as its summary. To extract a good summary from a long text document, sentence embedding plays an important role. Recent studies have leveraged graph neural networks to capture the inter-sentential relationship (e.g., the discourse graph) within the documents to learn contextual sentence embedding. However, those approaches neither consider multiple types of inter-sentential relationships (e.g., semantic similarity and natural connection relationships), nor model intra-sentential relationships (e.g, semantic similarity and syntactic relationship among words). To address these problems, we propose a novel Multiplex Graph Convolutional Network (Multi-GCN) to jointly model different types of relationships among sentences and words. Based on Multi-GCN, we propose a Multiplex Graph Summarization (Multi-GraS) model for extractive text summarization. Finally, we evaluate the proposed models on the CNN/DailyMail benchmark dataset to demonstrate effectiveness of our method.",
            doi: "10.18653/v1/2021.emnlp-main.11",
            year: 2021,
            month: 11,
            type: "Conference",
            venue: "EMNLP",
            volume: "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
            source_url: "https://aclanthology.org/2021.emnlp-main.11/",
            pdf_url: "https://aclanthology.org/2021.emnlp-main.11.pdf",
            ebook_url: "https://aclanthology.org/2021.emnlp-main.11.pdf",
            ppt_url: "https://www.slideserve.com/vera/text-summarization",
            video_url: "https://aclanthology.org/2021.emnlp-main.11.mp4",
            citations_num: 31,         // 被引次数
            // reference_list: [ref1, ref2, ...]     // 参考文献列表
        },
        {
            id: "6",
            title: "Multiplex Graph Neural Network for Extractive Text Summarization 6",
            authors: ["Baoyu Jing", "Zeyu You", "Tao Yang", "Wei Fan", "Hanghang Tong"],
            abstract: "Extractive text summarization aims at extracting the most representative sentences from a given document as its summary. To extract a good summary from a long text document, sentence embedding plays an important role. Recent studies have leveraged graph neural networks to capture the inter-sentential relationship (e.g., the discourse graph) within the documents to learn contextual sentence embedding. However, those approaches neither consider multiple types of inter-sentential relationships (e.g., semantic similarity and natural connection relationships), nor model intra-sentential relationships (e.g, semantic similarity and syntactic relationship among words). To address these problems, we propose a novel Multiplex Graph Convolutional Network (Multi-GCN) to jointly model different types of relationships among sentences and words. Based on Multi-GCN, we propose a Multiplex Graph Summarization (Multi-GraS) model for extractive text summarization. Finally, we evaluate the proposed models on the CNN/DailyMail benchmark dataset to demonstrate effectiveness of our method.",
            doi: "10.18653/v1/2021.emnlp-main.11",
            year: 2021,
            month: 11,
            type: "Conference",
            venue: "EMNLP",
            volume: "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
            source_url: "https://aclanthology.org/2021.emnlp-main.11/",
            pdf_url: "https://aclanthology.org/2021.emnlp-main.11.pdf",
            ebook_url: "https://aclanthology.org/2021.emnlp-main.11.pdf",
            ppt_url: "https://www.slideserve.com/vera/text-summarization",
            video_url: "https://aclanthology.org/2021.emnlp-main.11.mp4",
            citations_num: 12,         // 被引次数
            // reference_list: [ref1, ref2, ...]     // 参考文献列表
        },
        {
            id: "7",
            title: "Multiplex Graph Neural Network for Extractive Text Summarization 7",
            authors: ["Baoyu Jing", "Zeyu You", "Tao Yang", "Wei Fan", "Hanghang Tong"],
            abstract: "Extractive text summarization aims at extracting the most representative sentences from a given document as its summary. To extract a good summary from a long text document, sentence embedding plays an important role. Recent studies have leveraged graph neural networks to capture the inter-sentential relationship (e.g., the discourse graph) within the documents to learn contextual sentence embedding. However, those approaches neither consider multiple types of inter-sentential relationships (e.g., semantic similarity and natural connection relationships), nor model intra-sentential relationships (e.g, semantic similarity and syntactic relationship among words). To address these problems, we propose a novel Multiplex Graph Convolutional Network (Multi-GCN) to jointly model different types of relationships among sentences and words. Based on Multi-GCN, we propose a Multiplex Graph Summarization (Multi-GraS) model for extractive text summarization. Finally, we evaluate the proposed models on the CNN/DailyMail benchmark dataset to demonstrate effectiveness of our method.",
            doi: "10.18653/v1/2021.emnlp-main.11",
            year: 2021,
            month: 11,
            type: "Conference",
            venue: "EMNLP",
            volume: "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
            source_url: "https://aclanthology.org/2021.emnlp-main.11/",
            pdf_url: "https://aclanthology.org/2021.emnlp-main.11.pdf",
            ebook_url: "https://aclanthology.org/2021.emnlp-main.11.pdf",
            ppt_url: "https://www.slideserve.com/vera/text-summarization",
            video_url: "https://aclanthology.org/2021.emnlp-main.11.mp4",
            citations_num: 31,         // 被引次数
            // reference_list: [ref1, ref2, ...]     // 参考文献列表
        },
        {
            id: "8",
            title: "Multiplex Graph Neural Network for Extractive Text Summarization 8",
            authors: ["Baoyu Jing", "Zeyu You", "Tao Yang", "Wei Fan", "Hanghang Tong"],
            abstract: "Extractive text summarization aims at extracting the most representative sentences from a given document as its summary. To extract a good summary from a long text document, sentence embedding plays an important role. Recent studies have leveraged graph neural networks to capture the inter-sentential relationship (e.g., the discourse graph) within the documents to learn contextual sentence embedding. However, those approaches neither consider multiple types of inter-sentential relationships (e.g., semantic similarity and natural connection relationships), nor model intra-sentential relationships (e.g, semantic similarity and syntactic relationship among words). To address these problems, we propose a novel Multiplex Graph Convolutional Network (Multi-GCN) to jointly model different types of relationships among sentences and words. Based on Multi-GCN, we propose a Multiplex Graph Summarization (Multi-GraS) model for extractive text summarization. Finally, we evaluate the proposed models on the CNN/DailyMail benchmark dataset to demonstrate effectiveness of our method.",
            doi: "10.18653/v1/2021.emnlp-main.11",
            year: 2021,
            month: 11,
            type: "Conference",
            venue: "EMNLP",
            volume: "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
            source_url: "https://aclanthology.org/2021.emnlp-main.11/",
            pdf_url: "https://aclanthology.org/2021.emnlp-main.11.pdf",
            ebook_url: "https://aclanthology.org/2021.emnlp-main.11.pdf",
            ppt_url: "https://www.slideserve.com/vera/text-summarization",
            video_url: "https://aclanthology.org/2021.emnlp-main.11.mp4",
            citations_num: 12,         // 被引次数
            // reference_list: [ref1, ref2, ...]     // 参考文献列表
        },
        {
            id: "9",
            title: "Multiplex Graph Neural Network for Extractive Text Summarization 9",
            authors: ["Baoyu Jing", "Zeyu You", "Tao Yang", "Wei Fan", "Hanghang Tong"],
            abstract: "Extractive text summarization aims at extracting the most representative sentences from a given document as its summary. To extract a good summary from a long text document, sentence embedding plays an important role. Recent studies have leveraged graph neural networks to capture the inter-sentential relationship (e.g., the discourse graph) within the documents to learn contextual sentence embedding. However, those approaches neither consider multiple types of inter-sentential relationships (e.g., semantic similarity and natural connection relationships), nor model intra-sentential relationships (e.g, semantic similarity and syntactic relationship among words). To address these problems, we propose a novel Multiplex Graph Convolutional Network (Multi-GCN) to jointly model different types of relationships among sentences and words. Based on Multi-GCN, we propose a Multiplex Graph Summarization (Multi-GraS) model for extractive text summarization. Finally, we evaluate the proposed models on the CNN/DailyMail benchmark dataset to demonstrate effectiveness of our method.",
            doi: "10.18653/v1/2021.emnlp-main.11",
            year: 2021,
            month: 11,
            type: "Conference",
            venue: "EMNLP",
            volume: "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
            source_url: "https://aclanthology.org/2021.emnlp-main.11/",
            pdf_url: "https://aclanthology.org/2021.emnlp-main.11.pdf",
            ebook_url: "https://aclanthology.org/2021.emnlp-main.11.pdf",
            ppt_url: "https://www.slideserve.com/vera/text-summarization",
            video_url: "https://aclanthology.org/2021.emnlp-main.11.mp4",
            citations_num: 31,         // 被引次数
            // reference_list: [ref1, ref2, ...]     // 参考文献列表
        },
        {
            id: "10",
            title: "Multiplex Graph Neural Network for Extractive Text Summarization 10",
            authors: ["Baoyu Jing", "Zeyu You", "Tao Yang", "Wei Fan", "Hanghang Tong"],
            abstract: "Extractive text summarization aims at extracting the most representative sentences from a given document as its summary. To extract a good summary from a long text document, sentence embedding plays an important role. Recent studies have leveraged graph neural networks to capture the inter-sentential relationship (e.g., the discourse graph) within the documents to learn contextual sentence embedding. However, those approaches neither consider multiple types of inter-sentential relationships (e.g., semantic similarity and natural connection relationships), nor model intra-sentential relationships (e.g, semantic similarity and syntactic relationship among words). To address these problems, we propose a novel Multiplex Graph Convolutional Network (Multi-GCN) to jointly model different types of relationships among sentences and words. Based on Multi-GCN, we propose a Multiplex Graph Summarization (Multi-GraS) model for extractive text summarization. Finally, we evaluate the proposed models on the CNN/DailyMail benchmark dataset to demonstrate effectiveness of our method.",
            doi: "10.18653/v1/2021.emnlp-main.11",
            year: 2021,
            month: 11,
            type: "Conference",
            venue: "EMNLP",
            volume: "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
            source_url: "https://aclanthology.org/2021.emnlp-main.11/",
            pdf_url: "https://aclanthology.org/2021.emnlp-main.11.pdf",
            ebook_url: "https://aclanthology.org/2021.emnlp-main.11.pdf",
            ppt_url: "https://www.slideserve.com/vera/text-summarization",
            video_url: "https://aclanthology.org/2021.emnlp-main.11.mp4",
            citations_num: 12,         // 被引次数
            // reference_list: [ref1, ref2, ...]     // 参考文献列表
        }
    ];
    let hitList = [
        {
            page_url: "www.baidu.com",
            title: "百度",
            content: "后来加了参数，mockjs好像不能识别了，返回404.这个该怎么解决，真的不能加参数吗，但是我总感觉这个不太对"
        },
        {
            page_url: "www.baidu.com",
            title: "百度",
            content: "后来加了参数，mockjs好像不能识别了，返回404.这个该怎么解决，真的不能加参数吗，但是我总感觉这个不太对"
        }
    ]
    
    return {
        page: 1,                                   // 当前第几页（从1开始）
        searchCostTime: 0.01, // 搜索用时（单位为秒）
        totalNums: 10,
        pageNums: 1,
        hitList: hitList_new
    }
})

mock(RegExp('http://mock/api/detail' + '.*'), 'get', function () {
    console.log("sdasdasdasdasdadadasda!!!!!!!!")
    let detail = {
        id: "1",
        title: "Multiplex Graph Neural Network for Extractive Text Summarization",
        authors: ["Baoyu Jing", "Zeyu You", "Tao Yang", "Wei Fan", "Hanghang Tong"],
        abstract: "Extractive text summarization aims at extracting the most representative sentences from a given document as its summary. To extract a good summary from a long text document, sentence embedding plays an important role. Recent studies have leveraged graph neural networks to capture the inter-sentential relationship (e.g., the discourse graph) within the documents to learn contextual sentence embedding. However, those approaches neither consider multiple types of inter-sentential relationships (e.g., semantic similarity and natural connection relationships), nor model intra-sentential relationships (e.g, semantic similarity and syntactic relationship among words). To address these problems, we propose a novel Multiplex Graph Convolutional Network (Multi-GCN) to jointly model different types of relationships among sentences and words. Based on Multi-GCN, we propose a Multiplex Graph Summarization (Multi-GraS) model for extractive text summarization. Finally, we evaluate the proposed models on the CNN/DailyMail benchmark dataset to demonstrate effectiveness of our method.",
        doi: "10.18653/v1/2021.emnlp-main.11",
        year: 2021,
        month: 11,
        type: "Conference",
        venue: "EMNLP",
        volume: "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
        source_url: "https://aclanthology.org/2021.emnlp-main.11/",
        pdf_url: "https://aclanthology.org/2021.emnlp-main.11.pdf",
        ebook_url: "https://aclanthology.org/2021.emnlp-main.11.pdf",
        ppt_url: "https://www.slideserve.com/vera/text-summarization",
        video_url: "https://aclanthology.org/2021.emnlp-main.11.mp4",
        citations_num: 31,         // 被引次数
        reference_list: [          // 参考文献列表
            {id: 0, title: "Multiplex Graph Neural Network for Extractive Text Summarization", year: "2021"}, 
            {id: 4, title: "Multiplex Graph Neural Network for Extractive Text Summarization", year: "2021"}, 
            {id: 5, title: "Multiplex Graph Neural Network for Extractive Text Summarization", year: "2021"}]     
    }
    return detail
})

// //使用mockjs模拟数据
// mock('http://mock/api/headimage', 'get', function () {//当post或get请求到/api/data路由时Mock会拦截请求并返回上面的数据
//     let imgUrl = ['http://122.114.62.128:8003/Images/banner0.png', 'http://122.114.62.128:8003/Images/banner1.png']

//     return {
//         data: imgUrl
//     }
// })